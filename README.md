## Mohammad Mirzanejad

## Machine Learning Models Pure Implementation in Python

- In this repository I try to share my pure implementations of various machine learning algorithms.
- Although some models have been coded with some libraries, I will add pure implementation in the future, so stay tuned!

### Classification

- *Decision Tree:
-- It is a supervised learning algorithm that possesses the capability to perform both classification and regression tasks. The primary objective of employing a Decision Tree is to construct a training model that can accurately anticipate the class or value of the target variable based on decision rules inferred from the training data. Consequently, in a decision tree, a node symbolizes an attribute, each branch indicates a decision rule, and each leaf signifies an outcome.
The algorithm process consists of three main steps: 1. partitions the dataset based on various factors 2. Entails the removal of branches that utilize attributes of low importance 3. Determine the tree that aligns well with the data, as specified by the cross-validated error.

- *K-NN:
--- The K-nearest neighbors algorithm is a supervised machine learning method that is simple and easy to implement. It can be used for both classification and regression tasks, assuming that similar data points are close to each other in the scatter plot. To choose the right value for K, it is recommended to try out different values before settling on one. Low values of K can be noisy and subject to outliers, while large values of K may result in certain categories being consistently voted out. Therefore, it is important to select a value of K that minimizes errors, typically an odd number to break ties. The algorithm itself involves several steps, such as initializing K, calculating distances between data points, sorting the distances, and selecting the K nearest neighbors. For regression tasks, the mean of the K labels is returned, while for classification tasks, the mode of the K labels is returned. The algorithm also allows for seamless addition of new data without affecting its accuracy. However, it does not perform well with large datasets due to the high computational cost of distance calculations. Additionally, it is sensitive to noisy data, missing values, and outliers.

- *Naive Bayes:
--- Naive Bayes is a powerful classification algorithm based on Bayes's Theorem. It assumes that the presence of a feature in a class is independent of the presence of any other feature. The different types of Naive Bayes classifiers are Multinomial, Bernoulli, and Gaussian. The advantages of Naive Bayes include scalability, resistance to overfitting, and effectiveness in text classification. However, it has disadvantages such as the "Zero Conditional Probability Problem" and the strong assumption of independence between class features.

- *Random Forest:
--- Random forest is a well-known supervised learning technique in machine learning that is utilized for classification and regression tasks. It employs the concept of ensemble learning, which involves combining multiple classifiers to address complex problems and enhance model performance. The random forest algorithm utilizes the bagging technique, where multiple base models are trained on subsets of the dataset with replacement. The models in random forest are decision trees, and the algorithm consists of two phases: creating the random forest by combining N decision trees and making predictions for each tree. The algorithm selects random samples from the dataset using the bootstrap technique and constructs a decision tree for each sample, obtaining prediction results from each tree. These results are then aggregated through voting, with the most voted prediction serving as the final result. For a random forest classifier, the majority vote determines the final prediction, while a random forest regressor computes the mean of the decision tree results. Random forest offers advantages such as handling missing values, maintaining accuracy, and preventing overfitting. However, it may not be as suitable for regression tasks, and there is limited control over the model's behavior.

- *Support Vector Machines:
--- SVM is a supervised learning algorithm that tunes mathematical equations to optimize classification, regression, and outliers detection. A basic linear SVM classifier separates data points into different categories using a straight line. There are infinite possibilities for the placement of this line. There are two main types of SVM: simple SVM and kernel SVM. Simple SVM is used for linear regression and classification problems, while kernel SVM allows for more flexibility in handling non-linear data. SVM uses mathematical functions called kernels to find hyperplanes or linear decision boundaries in higher dimensions that aid in classification. There are different types of kernels, including linear, polynomial, RBF, and sigmoid, each with their own characteristics and applications. The regularization parameter (C) and the gamma parameter influence the optimization process of SVM, determining the trade-off between the margin of the hyperplane and the number of misclassified points.

### Regression

- *Linear Regresssion:
--- Regression is a technique used to predict a goal value by utilizing independent predictors. This method is commonly employed for forecasting and determining causal relationships between variables. The variation in regression techniques is mainly influenced by the number of independent variables and the nature of the relationship between them and the dependent variable. Linear regression can be divided into two types: simple linear regression, which uses one independent variable to predict a numerical dependent variable, and multiple linear regression, which employs multiple independent variables to estimate the value of a numerical dependent variable. The cost function, also known as the mean squared error (MSE) function, is utilized in linear regression to find the best fit line by minimizing the discrepancy between expected and actual values. Gradient descent is a method that iteratively adjusts the values of a0 and a1 in order to reduce the cost function and minimize loss. The gradient descent algorithm takes steps in the direction of the negative gradient to reach the minima, with the learning rate determining the speed of convergence.

- *Logistic Regression:
--- Logistic regression is a classification algorithm utilized for assigning observations to discrete classes, such as email spam or not spam, online transaction fraud or not fraud, and tumor malignant or benign. It employs the logistic sigmoid function to transform its output into a probability value. Logistic regression is applied to a different class of problems known as classification problems, where the objective is to predict the group to which the observed object belongs. It yields a discrete binary outcome ranging from 0 to 1. There are two types of logistic regression: binary (e.g., tumor malignant or benign) and multi-linear functions (e.g., cats, dogs, or sheep). The logistic regression model employs a more complex cost function, known as the sigmoid function, instead of a linear function. The decision boundary is determined by a threshold value that classifies values into different classes. The cost function, representing the optimization objective, is minimized to develop an accurate model with minimum error. Gradient descent is utilized to reduce the cost value and minimize the cost function. Logistic regression has several advantages, including ease of implementation, interpretation, and efficiency in training, no assumptions about class distributions, fast classification of unknown records, and good accuracy for simple datasets with linear separability. However, it has limitations, such as overfitting with fewer observations than features, construction of linear boundaries, inability to solve non-linear problems due to a linear decision surface, and the assumption of linearity between dependent and independent variables.

- *Multiple Linear Variation:
--- Multiple Linear Regression is an extension of Simple Linear Regression where the target variable is predicted using multiple predictor variables. The equation for multiple linear regression involves coefficients and independent variables. Assumptions for multiple linear regression include a linear relationship between the variables and normally distributed residuals. Advantages of multiple linear regression include simplicity and interpretability, while disadvantages include susceptibility to outliers and limited description of variable relationships.

- *Lasso:
--- The Lasso Regression, also known as L1 regularization, was introduced in 1996 by Professor Robert Tibshirani. It performs feature selection and regularization of data models by minimizing the loss function and the sum of the absolute value of the weights. This regression technique avoids overfitting and can provide accurate predictions, although the selected features may be biased. Different values of lambda can be used to adjust the trade-off between the number of selected features and the prediction error.

- *Ridge:
--- Ridge regression, also known as L2 regularization, is a form of Tikhonov regularization that regularizes all parameters equally. It was introduced by Hoerl and Kennard in 1970 for analyzing multiple regression data with multicollinearity. The objective of ridge regression is to minimize the loss function and the sum of squared coefficients. This is achieved by applying a regularization process that penalizes the regression variables' coefficients to minimize prediction error. Different values of lambda (λ) determine the level of regularization, with λ = 0 including all features, λ = ∞ excluding all features, and 0 < λ < ∞ providing weights between 0 and those of simple linear regression. Ridge regression has the advantage of avoiding overfitting and handling highly correlated features effectively. However, it cannot achieve feature selection or provide high model interpretability, and it increases bias.

- *Baysian network:
--- Bayesian Linear Regression, which is an approach to linear regression within the context of Bayesian inference, entails undertaking statistical analysis. In this approach, when the regression model has errors that follow a normal distribution, and a specific form of prior distribution is assumed, explicit results can be obtained for the posterior probability distributions of the model's parameters. Unlike traditional linear regression, Bayesian linear regression goes beyond estimating point estimates for the parameters. Instead, it takes into consideration the full posterior distribution over the parameters when making predictions. This means that instead of fitting specific parameter values, a mean is computed over all plausible parameter settings according to the posterior distribution. The posterior probability distribution of the model parameters given the input and the output can be represented as P( θ/y,x)=(P(y/ θ,x)*P( θ/x))/p(y/x), where P( θ/y,x) represents the posterior probability distribution. The posterior probability distribution can be calculated using the formula Posterior =(Likelihood*Prior)/Normalization. The Bayesian linear regression approach involves three main steps: creating a model that maps the training inputs to the training outputs, having a Markov Chain Monte Carlo (MCMC) algorithm to draw samples from the posterior distributions for the model parameters, and obtaining the posterior distribution for the parameters as the end result. The Bayesian Linear Regression framework has various applications that make it a valuable tool. It allows for the integration of prior data while still considering uncertainty. The Bayesian method, which is reflected in Bayesian Linear Regression, involves constructing an initial approximation and refining it as more evidence is gathered. This perspective is in line with the Bayesian philosophy of seeing the universe, where inference is a much better alternative to its frequentist counterpart.

### Clustering

- *K Means:
--- K-Means clustering is a popular unsupervised machine learning algorithm that aims to discover K clusters by finding K centroids and assigning each point to the nearest centroid. It works by starting with an initial set of cluster centroids, assigning each data point to its nearest centroid, and then re-computing the centroids. The algorithm terminates when certain conditions are met, such as no reduction in error or a predefined number of iterations. The advantages of K-Means include its linear complexity, simplicity of implementation, and adaptability to sparse data. However, it has disadvantages such as sensitivity to the initial partition or centroids, circular cluster requirements, sensitivity to noisy data and outliers, and the need for prior knowledge of the number of clusters.

- *DBSCAN:
--- The DBSCAN algorithm, which stands for Density-Based Spatial Clustering of Applications with Noise, is a clustering algorithm that can identify clusters of arbitrary shapes and clusters that contain outliers. It operates on the concept that a point belongs to a cluster if it is close to a significant number of other points in that cluster. The algorithm has two main parameters: epsilon (eps), which defines the radius of the neighborhood around a core point, and minPts, which specifies the minimum number of points required in a point's neighborhood. There are several advantages to using DBSCAN over other clustering algorithms. Unlike K-means, DBSCAN does not require prior knowledge of the number of clusters, and it only requires a distance threshold to define what is considered "close" for clustering. K-means clustering can sometimes group unrelated observations, and even if the results are far apart, they can still become part of a cluster due to the mean value calculation. DBSCAN mitigates this issue by creating clusters in a different way, resulting in reduced sensitivity to small shifts in data points. DBSCAN classifies points into three categories: core points, boundary points, and outliers or noise. Core points are surrounded by at least the minimum number of neighboring points within the specified radius. Boundary points can be reached from a core point, but they have fewer neighboring points than the minimum required. Outliers are points that do not meet the criteria for being a core or boundary point. Density edges can be formed between core points if their distance is within the epsilon value. Density connected points are core points that can be connected through a path of density edges. The steps for the DBSCAN algorithm involve classifying the points, discarding noise, assigning clusters to core points, and coloring the density connected points and boundary points accordingly.

- *Hierarchical Clustering:
--- Hierarchical clustering is an unsupervised machine learning algorithm that constructs a hierarchy of clusters. It begins by assigning each data point to its own cluster and then merges the two nearest clusters. The algorithm terminates when only one cluster remains. The results can be visualized using a dendrogram. The advantages of hierarchical clustering include not requiring prior knowledge of the number of clusters and being easy to implement. However, it has disadvantages such as the inability to undo previous actions, a time complexity of at least O(n2 log n), sensitivity to noise and outliers, difficulty handling different sized clusters and convex shapes, and the lack of a directly minimized objective function. Additionally, it can be challenging to determine the correct number of clusters from the dendrogram.

- *Gaussian Mixture Model:
--- Gaussian Mixture consists of several Gaussians, each identified by k ∈ {1,…, K}, where K is the number of clusters. Each Gaussian k has a mean µ, covariance ∑, and mixing probability π. For 3 Gaussian distributions with mean (µ1, µ2, µ3) and variance (σ1, σ2, σ3), GMM identifies the probability of data points belonging to each distribution. The Gaussian function is given by µ and σ2. A 3D Bell Curve can be shown, where the probability density function is defined by x, µ, and ∑. The covariance defines the shape of the curve, and this can be generalized for d-dimensions. The multivariate Gaussian model has x and µ as vectors and ∑ as a covariance matrix. The Expectation-Maximization algorithm is used for maximum likelihood estimation in the presence of latent variables. It uses existing data to determine the optimum values for these variables and updates the model parameters accordingly. The algorithm has two steps: (1) probabilistically assigning data points to clusters, and (2) updating cluster parameters based on assigned probabilities. Expectation-Maximization is the base for many algorithms, including Gaussian Mixture Models. 1. Data X and unobserved data Δ are considered in the Expectation Maximization framework. 2. The model involves parameters θ, and the log-likelihood ℓ(θ; X, Δ) can be computed to assess the probability of observing the data and the latent variables. 3. Additionally, the conditional distribution Δ|X can be computed using the model and parameters θ, denoted as P(Δ|X; θ). 4. As a result, the log-likelihood ℓ(θ; X) can be calculated based on the data alone, without assuming an assignment for the latent variables.

### Data Mining

- *Apriori:
--- The Apriori algorithm, developed to identify frequent itemsets in a boolean association rule dataset. By utilizing association rules, it determines the strength of the connection between two objects. This algorithm employs a breadth-first search and Hash Tree to efficiently calculate itemset associations. The algorithm follows an iterative process to find frequent itemsets in large datasets. To enhance efficiency, the Apriori Property is utilized, which reduces the search space. Frequent itemsets are those items with support greater than the threshold or minimum specified by the user. The Apriori Property states that all subsets of a frequent itemset must also be frequent. The Apriori algorithm has several advantages, such as its simplicity and ease of implementation on large datasets. However, it also has disadvantages, including slower execution compared to other algorithms and high time and space complexity.

- *FP-Growth:
--- The FP growth algorithm, an improvement of the apriori algorithm, overcomes the drawbacks of rebuilding candidate sets and repeatedly scanning the database by storing transactions in a tree data structure. This compressed representation, known as the FP tree, maps itemsets to paths in the tree and maintains the association between them. The tree is mined recursively to generate frequent patterns, and pattern growth is achieved by concatenating patterns from conditional FP trees. The FP growth algorithm scans the database only twice, compared to the Apriori algorithm, making it faster and more memory-efficient for mining both long and short frequent patterns. However, building the FP tree can be more cumbersome and expensive, and the algorithm may not be suitable for large databases that do not fit in shared memory.

### Neural Networks

- *Perceptron:
--- A Perceptron is a single-layer neural network used for supervised learning of binary classifiers. It takes inputs X, Y, and Z, which are weighted by coefficients w1, w2, and w3 respectively. The sum of these weighted inputs, Z, determines if the neuron fires. This firing depends on an activation function, and if the sum exceeds a threshold, the neuron outputs a signal. The Perceptron function maps its input x to an output f(x) based on the learned weight coefficients and a bias term. The output can be either 1 or 0, or 1 or -1 depending on the activation function used.

- *Multi-Layer Perceptron (MLP):
--- The MULTI-LAYER PERCEPTRON is a type of feedforward artificial neural network that consists of multiple layers of interconnected neurons. It is commonly used for tasks such as approximation, pattern classification, prediction, and recognition. The network is structured with an input layer, one or more hidden layers, and an output layer. The input layer receives the initial inputs, while the hidden layers perform the majority of the computations. The output layer is responsible for producing the classification or prediction outputs.  The calculations in a MULTI-LAYER PERCEPTRON occur at each neuron in the hidden or output layers. To explain this process, we can consider the previous neuron outputs as vector x, the weights of the edges connecting the previous layer neurons to the current layer are represented by matrix w, and the bias vector is denoted as b. Therefore, the current layer's output h is determined by the activation function G(wTx+b), where G(x) is typically a sigmoid function. After calculating the values of h for each layer until the output layer, the next step is to perform backpropagation in order to learn the adjustable factors such as weights and biases. During backpropagation, the loss at the output layer is determined, and this information is used to update the factors in each previous layer based on how much the loss changes. This iterative process allows the MULTI-LAYER PERCEPTRON to refine its predictions and improve its performance.

- *Deep Neural Network:
--- A basic deep neural network is a type of artificial neural network that can learn complex patterns and relationships from data. It consists of multiple layers of nodes, each performing some mathematical operation on the input and passing the output to the next layer. The first layer is called the input layer, and the last layer is called the output layer. The layers in between are called hidden layers, because they are not directly visible to the outside. A deep neural network can have many hidden layers, hence the name “deep”. A feed forward neural network is a special case of a deep neural network, where the information flows only in one direction, from the input layer to the output layer. There are no loops or cycles in the network, unlike in recurrent neural networks. A feed forward neural network is also known as a multi-layer perceptron, because it is composed of multiple layers of perceptrons. A perceptron is a simple node that takes a weighted sum of its inputs, adds a bias term, and applies a non-linear activation function to produce an output. The basic steps to implement a feed forward neural network are: Define the network architecture, such as the number of layers, the number of nodes in each layer, and the activation function for each node. Initialize the weights and biases of the network randomly or with some heuristic. Feed the input data to the network and compute the output of each layer using the formula: output=activation(weight.input+bias). Compare the output of the network with the desired output (the ground truth labels) and calculate the error or loss function, such as mean squared error or cross entropy. Backpropagate the error through the network and update the weights and biases using some optimization algorithm, such as gradient descent or stochastic gradient descent. Repeat steps 3 to 5 until the network converges to a satisfactory level of accuracy or a maximum number of iterations is reached. A feed forward neural network can be used for various tasks, such as classification, regression, dimensionality reduction, and feature extraction. It can also be combined with other types of neural networks, such as convolutional neural networks or recurrent neural networks, to form more complex and powerful models.

### Deep Learning

- *Convolutional Neural Network:
--- A convolutional neural network (CNN) is an artificial neural network that can extract features from data, particularly images, by applying filters or kernels to the input. The CNN consists of various types of layers, including convolutional, pooling, and fully-connected layers, which perform different tasks on the data. Training a CNN involves defining the network architecture, preparing the data, training the network, evaluating its performance, and fine-tuning it. CNNs possess the advantage of automatically learning features from data without manual extraction, capturing spatial and temporal dependencies, achieving high accuracy and generalization, but also have disadvantages such as high computational requirements, difficulty in interpretation and explanation, and sensitivity to hyperparameters and network architecture.

- *Recurrent Neural Network:
--- A recurrent neural network (RNN) is an artificial neural network capable of processing sequential data using memory to retain information from previous inputs. Unlike a feedforward neural network, an RNN can transmit information in both directions, capturing temporal dependencies and context. The implementation of an RNN involves defining the network architecture, preparing the data, training the network, evaluating its performance, and fine-tuning it. RNNs have advantages in modeling complex and dynamic sequences, learning long-term dependencies, and handling variable-length inputs and outputs. However, they also have disadvantages such as high computational requirements, the vanishing or exploding gradient problem, and a tendency to overfit.

- *Long Short Term Memory:
--- An LSTM neural network is a type of RNN that can process sequential data by using its internal state to store information from previous inputs. Unlike a vanilla RNN, an LSTM has a complex structure and can overcome the vanishing or exploding gradient problem. An LSTM can learn long-term dependencies and context, which makes it suitable for tasks such as natural language processing, speech recognition, and video analysis. The steps to implement an LSTM are similar to those of an RNN, except that the network architecture is different. An LSTM cell has four components: a cell state, an input gate, a forget gate, and an output gate. The advantages of LSTM are that they can model complex and dynamic sequences, learn long-term dependencies and context, and handle variable-length inputs and outputs. The disadvantages are that they require a lot of computational resources and memory, can be difficult to interpret and explain, and can be sensitive to hyperparameters and network architecture. The differences from RNN are that LSTM has a more complex structure, can overcome the vanishing or exploding gradient problem, and can learn long-term dependencies and context better.

- *GAN:
--- GANs use deep neural networks to learn from complex and high-dimensional data, such as images, speech, or text. GANs are generative models that can generate new examples from a given training set, by using two networks: a generator and a discriminator. The generator tries to fool the discriminator by producing realistic samples, while the discriminator tries to distinguish between real and fake samples. The two networks are trained in an adversarial manner, until they reach an equilibrium where the generator produces plausible samples and the discriminator cannot tell them apart from the real ones.


### Dimention reduction
- *LDA:
--- Dimensionality reduction is an effective technique for reducing the dimensions of data features in order to prevent overfitting. Linear Discriminant Analysis (LDA) is one such technique that aims to reduce the number of dimensions in a dataset while retaining as much information as possible. LDA is a supervised classification technique commonly used in biometrics, bioinformatics, and chemistry. It involves three steps: calculating the separability between different classes, determining the distance between the mean and sample of each class, and constructing a lower dimensional space that maximizes between-class variance and minimizes within-class variance. When preparing data for LDA, it is important to consider classification problems, assume a Gaussian distribution, remove outliers, and ensure that each input variable has the same variance. LDA also has variations such as Regularized Discriminant Analysis (RDA), Quadratic Discriminant Analysis (QDA), and Flexible Discriminant Analysis (FDA), which offer different approaches to dimensionality reduction.

- *PCA:
--- Principal Component Analysis (PCA) is a widely used unsupervised algorithm for dimensionality reduction. Its main objective is to identify and detect correlations between variables. The algorithm works by standardizing the data, calculating the covariance matrix, finding the eigenvectors and eigenvalues, sorting them in descending order, creating a projection matrix, and transforming the original dataset to a lower-dimensional feature subspace. PCA offers several advantages, including the ability to delete related features, reduce excess variation, improve visualization, and enhance algorithm performance. However, it also has some disadvantages, such as potential data loss, difficulty in interpreting independent variables, and the requirement for proper data preprocessing.

### Evolutionary algorithms

- *Genetic Algorithm:
--- Genetic Algorithm (GA) is an optimization technique that utilizes the principles of Genetics and Natural Selection. It leverages historical data and random search to find high-quality solutions for optimization and search problems. GA is inspired by the process of natural selection in finding the best solution. The GA block diagram consists of important parameters such as fitness assignment, selection, crossover, and mutation. Fitness assignment determines the preference for individuals with lower fitness values. Selection allows individuals with good fitness scores to pass their genes to successive generations. Crossover reorganizes information from two individuals to create a new offspring, and mutation protects against loss of potentially useful genetic material. GAs have advantages such as not requiring derivative information, being faster and more efficient than traditional methods, having good parallel capabilities, optimizing both continuous and discrete functions, providing multiple good solutions, and continuously improving the solution. However, GAs have limitations, including not being suitable for simple problems with available derivative information, computationally expensive fitness value calculations, no guarantees on optimality or solution quality, and the possibility of not converging to the optimal solution if not implemented properly.

- *Ant Colony:
--- An Ant Colony System is a form of optimization algorithm that emulates the behavior of real ants in their quest to find the shortest routes between their dwelling and food sources. This technique is grounded in the concept that ants communicate with one another by releasing pheromones on the ground, which subsequently influence the likelihood of other ants following the same path. Through this mechanism, ants collectively discover and reinforce the most optimal solutions over time. The fundamental components of an Ant Colony System consist of the following: A collection of artificial ants, each equipped with a memory and a position within the search space. - A pheromone matrix that stores the quantity of pheromone present on each edge of the search space. - A heuristic function that evaluates the quality of each edge within the search space. - A set of parameters, such as the number of ants, the rate of pheromone evaporation, and the relative importance assigned to both pheromone and heuristic information. The essential steps involved in implementing an Ant Colony System are as follows: 1. Initialization: Begin by setting the pheromone matrix to a small positive value and randomly placing the ants within the search space. 2. Solution Construction: For each ant, construct a solution by iteratively selecting the subsequent edge to traverse. This selection is based on a probabilistic rule that takes into account both the pheromone and heuristic information. 3. Solution Evaluation and Pheromone Update: Assess the quality of each solution and update the pheromone matrix accordingly. This involves increasing the amount of pheromone on edges that belong to the best solutions while simultaneously decreasing the amount on other edges. 4. Iteration: Repeat steps 2 and 3 until a termination criterion is met. This criterion may involve reaching a maximum number of iterations or achieving a satisfactory level of quality. An Ant Colony System can be effectively employed for a range of problems, including the traveling salesman problem, vehicle routing problem, quadratic assignment problem, and graph coloring problem. Furthermore, it can be combined with other techniques, such as local search, to enhance its overall performance.

### Optimization

- *Stochastic Gradient Descent:
--- Stochastic gradient descent (SGD) is commonly employed for regression problems involving very large datasets, typically in the millions. While SGD shares similarities with the gradient descent algorithm, it differs in terms of the optimization function used. The basic concept behind SGD is that it operates as an iterative algorithm, starting from a random point within the dataset and sequentially fitting the training sets. This process entails initializing the theta (weights) with random values and then modifying them based on each dataset, with the loop continuing until the last dataset is processed. The intuition behind SGD involves shuffling the data to eliminate any initial pattern and progressively improving the accuracy of fitting subsequent data. In contrast to gradient descent (batch descent), which iterates from 1 to m (where m represents the number of datasets) in each iteration, SGD iterates over one dataset at a time. The main advantages of SGD include computational efficiency and the ability to effectively train models with large datasets. However, SGD is not as effective when working with smaller datasets.


### Probablistic Models
- *Matkov's Chain:
---  Markov's Chain is a stochastic process consisting of random variables that transition from one state to another based on probabilistic rules and certain assumptions. This transition is governed by the Markov Property, which states that the probability of transitioning to the next state only depends on the current state and time, independent of prior states. Mathematically, a Markov chain is represented by a sequence of random variables {Xm, m=0,1,2,⋯}, and it satisfies a specific formula. If the number of states is finite, it is called a finite Markov chain. A Markov model is visually represented by a State Transition Diagram, illustrating the transitions between states. Real-world applications of Markov chains include Google PageRank, word prediction, subreddit simulation, and text generation.

- *Hidden Markov Model:
--- A Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with concealed states. The utilization of an HMM enables us to discuss both observed events, such as words that are visible in the input, and hidden events, such as Part-Of-Speech tags. The components that specify an HMM consist of State Transition Probabilities, which determine the likelihood of transitioning from one state to another. The Observation Probability Matrix, also known as emission probabilities, expresses the probability of an observation being generated from a particular state. Finally, the Initial State Distribution, denoted as πi, represents the probability that the Markov chain will begin in state i. It is important to note that if the probability of a certain state, denoted as πj, equals zero, it signifies that it cannot be an initial state. Consequently, the entire structure of the Hidden Markov Model can be accurately described in this manner.

### Reinforcement Learning

Reinforcement learning (RL) is a branch of machine learning that studies how an agent can learn to interact with an environment by trial and error, in order to maximize a reward signal. The agent does not have prior knowledge of the environment's dynamics, but learns from its own experience and feedback. RL can be used to solve complex and dynamic problems that require adaptive and optimal decision making.

- *Temporal difference (TD) learning:
--- A class of model-free RL methods that learn by bootstrapping from the current estimate of the value function, which represents the expected future reward of a state or a state-action pair. TD methods update their estimates based on the difference between the current and the next prediction, without waiting for the final outcome. TD methods can handle incomplete and delayed feedback, and can learn online and offline. ¹

- *Q-learning: 
--- A model-free RL algorithm that learns the optimal action-value function, which represents the expected future reward of taking a certain action in a certain state, regardless of the policy being followed. Q-learning uses the Bellman equation to iteratively update the Q-values based on the observed rewards and the maximum Q-value for the next state. Q-learning is an off-policy method, meaning that it can learn from any experience, not necessarily the one generated by the current policy. ²

- *SARSA: 
--- A model-free RL algorithm that learns the action-value function for a given policy, which represents the expected future reward of taking a certain action in a certain state, following the same policy. SARSA uses the same update rule as Q-learning, but instead of taking the maximum Q-value for the next state, it takes the Q-value corresponding to the action that is actually taken. SARSA is an on-policy method, meaning that it learns from the experience generated by the current policy. ³

- *Expected SARSA:
--- A model-free RL algorithm that is similar to SARSA, but instead of taking the Q-value corresponding to the action that is actually taken, it takes the expected Q-value over all possible actions, weighted by their probability under the current policy. Expected SARSA reduces the variance of the update and can achieve better performance than SARSA. Expected SARSA can also be seen as a generalization of Q-learning, where the exploration policy is not necessarily greedy. ⁴

- *Dyna-Q: 
--- A model-based RL algorithm that combines direct RL with planning. Direct RL is the process of learning from real experience, as in Q-learning or SARSA. Planning is the process of learning from simulated experience, generated by a model of the environment. Dyna-Q uses both real and simulated experience to update the Q-values, and alternates between acting, learning a model, and planning. Dyna-Q can speed up learning and improve performance compared to model-free methods. ⁵

- *Actor-Critic:
--- The actor critic approach in reinforcement learning is a hybrid method that combines the advantages of both policy-based and value-based methods. Policy-based methods learn a policy that maps states to actions, while value-based methods learn a value function that estimates the expected return of states or actions. The actor critic method uses two components: an actor and a critic. The actor learns a policy that decides which action to take in each state, and the critic learns a value function that evaluates the quality of the actions taken by the actor. The actor and the critic interact and update each other, so that the actor can improve its policy based on the feedback from the critic, and the critic can improve its value function based on the experience from the actor. The actor critic method differs from other types of reinforcement learning methods in several ways: It differs from policy-based methods, because it uses a value function to reduce the variance of the policy gradient and to guide the exploration of the actor. It differs from value-based methods, such as Q-learning, because it uses a policy to directly control the actions of the agent and to avoid the curse of dimensionality and the overestimation bias of the value function. It differs from model-based methods, because it does not use a model of the environment to generate simulated experience, but relies on the actual experience from the environment.

- *DQN:
--- Deep Q-Network is a deep reinforcement learning (DRL) algorithm that combines Q-learning with deep neural networks to learn from complex and high-dimensional data, such as images, speech, or text. Q-learning is a value-based reinforcement learning (RL) algorithm that learns the optimal action-value function, which represents the expected future reward of taking a certain action in a certain state, regardless of the policy being followed. DQN uses a deep neural network to approximate the Q-function, and introduces two techniques to stabilize learning: experience replay and target network. Experience replay is a method of storing and sampling transitions from a replay buffer, to reduce the correlation and variance of the updates. Target network is a method of using a separate network to generate the target Q-values, and updating it periodically, to reduce the overestimation bias and the non-stationarity of the updates12

- *PG:
--- Policy Gradient is a policy-based RL algorithm that learns a stochastic policy that maximizes the expected return of a given task. PG uses the policy gradient theorem, which computes the gradient of the expected return with respect to the policy parameters, and updates the policy in the direction of the gradient. PG does not use a value function or a model of the environment, but relies on the actual rewards obtained from the environment. PG can handle discrete and continuous action spaces, and episodic and continuing tasks34

### Boosting

- *Adaboost:
--- AdaBoost, also known as Adaptive Boosting, is a widely used technique in boosting that aims to create a strong classifier by combining multiple weak classifiers. AdaBoost belongs to the ensemble machine learning models, which excel at combining different models to produce a more accurate meta model. This meta model achieves higher prediction accuracy compared to its individual counterparts. The AdaBoost algorithm falls under ensemble boosting techniques and consists of two phases: allowing weak learners to learn from training data and combining these models to generate a meta-model that resolves errors made by the individual weak learners. Boosting is an ensemble modeling technique that builds a strong classifier by using weak models in series. It starts with building a model from the training data and then builds subsequent models to correct errors made by the previous models. This process continues until the complete training data set is predicted correctly or the maximum number of models is reached. The working of the AdaBoost algorithm involves creating weak classifiers based on weighted samples, evaluating the performance of each classifier, assigning more weight to incorrectly classified samples, and iterating until all data points are correctly classified or the maximum iteration level is reached. The advantages of AdaBoost include the ability to combine low accuracy models into a high accuracy model, low generalization error, ease of implementation, and compatibility with a wide range of classifiers. However, it is susceptible to uniform noise and can lead to overfitting if the weak classifiers are too weak.
